#!/usr/bin/env python3
"""
FastAPIåç«¯æœåŠ¡å™¨ - OCRå¤„ç†API
æä¾›RESTful APIæ¥å£ç”¨äºå‰åç«¯åˆ†ç¦»çš„OCRå¤„ç†
"""

import os
import uuid
import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import threading
import time

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Request
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# å¯¼å…¥åŸæœ‰OCRåŠŸèƒ½
from main import (
    get_supported_files, get_terminology_files, load_terminology,
    get_file_page_count, ocr_manager
)
from file_splitter import PDFSplitter, SplitConfig, SplitStrategy, ChunkInfo, create_splitter_config


@dataclass
class ProcessingTask:
    """å¤„ç†ä»»åŠ¡æ•°æ®ç±»"""
    task_id: str
    files: List[str]
    start_page: int
    end_page: int
    ocr_service: str
    terminology: str
    status: str  # pending, analyzing, splitting, processing, merging, completed, failed, stopped
    progress: float
    current_file: str
    current_page: int
    total_pages: int
    total_tokens: int
    ocr_tokens: int
    gemini_tokens: int
    start_time: datetime
    end_time: Optional[datetime]
    error_message: Optional[str]
    log_messages: List[Dict[str, Any]]
    # æ–°å¢åˆ†å—ç›¸å…³å­—æ®µ
    chunks: List[ChunkInfo] = None
    current_chunk: int = 0
    total_chunks: int = 0
    split_config: Optional[Dict] = None
    file_analysis: Optional[Dict] = None


class SplitConfigRequest(BaseModel):
    """åˆ†å‰²é…ç½®è¯·æ±‚æ¨¡å‹"""
    strategy: str = "adaptive"  # adaptive, by_pages, by_size, by_memory, intelligent
    max_pages_per_chunk: int = 50
    max_size_per_chunk_mb: int = 100
    max_memory_usage_mb: int = 512
    enable_parallel: bool = True
    preserve_structure: bool = True


class ProcessRequest(BaseModel):
    """å¤„ç†è¯·æ±‚æ¨¡å‹"""
    filename: str
    start_page: int = 1
    end_page: int = 1
    ocr_service: str = "dashscope"
    terminology: str = ""
    # æ–°å¢åˆ†å‰²é…ç½®
    enable_splitting: bool = True
    split_config: Optional[SplitConfigRequest] = None
    # æ–°å¢é¢„å¤„ç†é…ç½®
    enable_preprocessing: bool = True
    preprocessing_mode: str = "document"


class UploadResponse(BaseModel):
    """ä¸Šä¼ å“åº”æ¨¡å‹"""
    filename: str
    size: int
    message: str


class FileInfo(BaseModel):
    """æ–‡ä»¶ä¿¡æ¯æ¨¡å‹"""
    name: str
    size: int
    pages: int
    upload_time: str


class SystemStatus(BaseModel):
    """ç³»ç»ŸçŠ¶æ€æ¨¡å‹"""
    available_ocr_services: Dict[str, str]
    terminology_files: List[str]
    pdf_files: List[FileInfo]  # ä¿æŒå…¼å®¹æ€§ï¼Œå®é™…åŒ…å«æ‰€æœ‰æ”¯æŒæ ¼å¼


# å…¨å±€çŠ¶æ€ç®¡ç†
tasks: Dict[str, ProcessingTask] = {}
task_stop_flags: Dict[str, threading.Event] = {}


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="OCRå¤„ç†API",
    description="å¤šå¼•æ“OCR + AIæ™ºèƒ½çº é”™æœåŠ¡",
    version="3.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™æ€æ–‡ä»¶æœåŠ¡ (ç”¨äºå‰ç«¯)
app.mount("/static", StaticFiles(directory="web"), name="static")


@app.get("/")
async def read_root():
    """æ ¹è·¯å¾„ - è¿”å›å‰ç«¯é¡µé¢"""
    return FileResponse("web/index.html")


@app.get("/api/status", response_model=SystemStatus)
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        # è·å–å¯ç”¨OCRæœåŠ¡
        available_services = ocr_manager.get_available_services()
        ocr_services = {
            key: service.get_description() 
            for key, service in available_services.items()
        }
        
        # è·å–ä¸“ä¸šè¯å…¸æ–‡ä»¶
        terminology_files = get_terminology_files()
        terminology_list = [f.name for f in terminology_files]
        
        # è·å–æ”¯æŒçš„æ–‡ä»¶ä¿¡æ¯
        input_files = get_supported_files()
        file_info_list = []
        
        for input_file in input_files:
            try:
                pages = get_file_page_count(input_file)
                size = input_file.stat().st_size
                upload_time = datetime.fromtimestamp(input_file.stat().st_mtime).isoformat()
                
                file_info_list.append(FileInfo(
                    name=input_file.name,
                    size=size,
                    pages=pages,
                    upload_time=upload_time
                ))
            except Exception as e:
                print(f"Error reading file info for {input_file}: {e}")
                continue
        
        return SystemStatus(
            available_ocr_services=ocr_services,
            terminology_files=terminology_list,
            pdf_files=file_info_list
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")


@app.post("/api/upload", response_model=UploadResponse)
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ - æ”¯æŒå¤§æ–‡ä»¶"""
    from format_processor import FormatProcessor
    
    # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
    processor = FormatProcessor()
    if not processor.is_supported(file.filename):
        supported_exts = processor.get_supported_extensions()
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(supported_exts)}"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å° (é™åˆ¶500MB)
    MAX_FILE_SIZE = 500 * 1024 * 1024  # 500MB
    
    try:
        # ç¡®ä¿inputç›®å½•å­˜åœ¨
        input_dir = Path("input")
        input_dir.mkdir(exist_ok=True)
        
        # ä¸ºé¿å…æ–‡ä»¶åå†²çªï¼Œæ·»åŠ æ—¶é—´æˆ³
        timestamp = int(time.time())
        file_path = input_dir / f"{timestamp}_{file.filename}"
        
        # åˆ†å—è¯»å–å’Œä¿å­˜æ–‡ä»¶ä»¥æ”¯æŒå¤§æ–‡ä»¶
        total_size = 0
        chunk_size = 8192  # 8KB chunks
        
        with open(file_path, "wb") as f:
            while True:
                chunk = await file.read(chunk_size)
                if not chunk:
                    break
                
                total_size += len(chunk)
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                if total_size > MAX_FILE_SIZE:
                    f.close()
                    file_path.unlink()  # åˆ é™¤éƒ¨åˆ†ä¸Šä¼ çš„æ–‡ä»¶
                    raise HTTPException(status_code=413, detail="æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ500MB")
                
                f.write(chunk)
        
        # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
        try:
            from main import get_file_page_count
            pages = get_file_page_count(file_path)
            if pages <= 0:
                file_path.unlink()
                raise HTTPException(status_code=400, detail="æ–‡ä»¶æŸåæˆ–ä¸ºç©º")
        except Exception as e:
            file_path.unlink()
            raise HTTPException(status_code=400, detail="æ–‡ä»¶æ— æ•ˆæˆ–æ ¼å¼ä¸æ”¯æŒ")
        
        return UploadResponse(
            filename=file_path.name,  # è¿”å›å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
            size=total_size,
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸ"
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")


@app.get("/api/file/{filename}")
async def get_file_info(filename: str):
    """è·å–æŒ‡å®šæ–‡ä»¶ä¿¡æ¯"""
    try:
        input_dir = Path("input")
        file_path = input_dir / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        pages = get_file_page_count(file_path)
        size = file_path.stat().st_size
        upload_time = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
        
        return {
            "name": file_path.name,
            "size": size,
            "pages": pages,
            "upload_time": upload_time
        }
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/api/analyze/{filename}")
async def analyze_file(filename: str):
    """åˆ†æPDFæ–‡ä»¶ç‰¹å¾å’Œåˆ†å‰²å»ºè®®"""
    try:
        input_dir = Path("input")
        file_path = input_dir / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åˆ›å»ºåˆ†å‰²å™¨å¹¶åˆ†ææ–‡ä»¶
        splitter = PDFSplitter()
        analysis = splitter.analyze_pdf(file_path)
        
        # åˆ›å»ºåˆ†å‰²è®¡åˆ’
        split_plan = splitter.create_split_plan(file_path, analysis)
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        result = {
            "filename": filename,
            "analysis": analysis,
            "split_plan": [
                {
                    "chunk_id": chunk.chunk_id,
                    "start_page": chunk.start_page,
                    "end_page": chunk.end_page,
                    "page_count": chunk.page_count,
                    "estimated_size_mb": round(chunk.estimated_size_mb, 2)
                }
                for chunk in split_plan
            ],
            "needs_splitting": analysis["needs_splitting"],
            "recommended_strategy": analysis["recommended_strategy"].value,
            "total_chunks": len(split_plan)
        }
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}")


@app.post("/api/process")
async def start_processing(request: ProcessRequest, background_tasks: BackgroundTasks):
    """å¼€å§‹å¤„ç†PDFæ–‡ä»¶"""
    try:
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        input_dir = Path("input")
        file_path = input_dir / request.filename
        
        if not file_path.exists():
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {request.filename}")
        
        # éªŒè¯OCRæœåŠ¡
        available_services = ocr_manager.get_available_services()
        if request.ocr_service not in available_services:
            raise HTTPException(status_code=400, detail=f"OCRæœåŠ¡ä¸å¯ç”¨: {request.ocr_service}")
        
        # åˆ›å»ºä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åŠ è½½ä¸“ä¸šè¯å…¸
        terminology_terms = ""
        if request.terminology:
            terminology_files = get_terminology_files()
            for term_file in terminology_files:
                if term_file.name == request.terminology:
                    terminology_terms = load_terminology(term_file)
                    break
        
        # åˆ›å»ºåˆ†å‰²é…ç½®
        split_config_dict = None
        if request.enable_splitting and request.split_config:
            split_config_dict = {
                "strategy": request.split_config.strategy,
                "max_pages_per_chunk": request.split_config.max_pages_per_chunk,
                "max_size_per_chunk_mb": request.split_config.max_size_per_chunk_mb,
                "max_memory_usage_mb": request.split_config.max_memory_usage_mb,
                "enable_parallel": request.split_config.enable_parallel,
                "preserve_structure": request.split_config.preserve_structure
            }
        
        # åˆ›å»ºé¢„å¤„ç†é…ç½®
        preprocessing_config = None
        if request.enable_preprocessing:
            from image_preprocessor import create_preprocessor_config
            preprocessing_config = create_preprocessor_config(mode=request.preprocessing_mode)
        
        # åˆ›å»ºå¤„ç†ä»»åŠ¡
        task = ProcessingTask(
            task_id=task_id,
            files=[request.filename],  # è½¬ä¸ºåˆ—è¡¨ä»¥ä¿æŒå…¼å®¹æ€§
            start_page=request.start_page,
            end_page=request.end_page,
            ocr_service=request.ocr_service,
            terminology=request.terminology,
            status="pending",
            progress=0.0,
            current_file="",
            current_page=0,
            total_pages=0,
            total_tokens=0,
            ocr_tokens=0,
            gemini_tokens=0,
            start_time=datetime.now(),
            end_time=None,
            error_message=None,
            log_messages=[],
            # æ–°å¢å­—æ®µ
            chunks=[],
            current_chunk=0,
            total_chunks=0,
            split_config=split_config_dict,
            file_analysis=None
        )
        
        # ä¿å­˜ä»»åŠ¡
        tasks[task_id] = task
        task_stop_flags[task_id] = threading.Event()
        
        # å¯åŠ¨åå°å¤„ç†
        background_tasks.add_task(
            process_file_background,
            task_id,
            request.filename,
            request.start_page,
            request.end_page,
            terminology_terms,
            request.ocr_service,
            request.enable_splitting,
            preprocessing_config
        )
        
        return {"task_id": task_id, "message": "å¤„ç†ä»»åŠ¡å·²å¯åŠ¨"}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å¤„ç†å¤±è´¥: {str(e)}")


@app.get("/api/progress/{task_id}")
async def get_progress(task_id: str):
    """è·å–å¤„ç†è¿›åº¦"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    return {
        "task_id": task_id,
        "status": task.status,
        "progress": task.progress,
        "current_file": task.current_file,
        "current_page": task.current_page,
        "total_pages": task.total_pages,
        "total_tokens": task.total_tokens,
        "ocr_tokens": task.ocr_tokens,
        "gemini_tokens": task.gemini_tokens,
        "start_time": task.start_time.isoformat(),
        "end_time": task.end_time.isoformat() if task.end_time else None,
        "error_message": task.error_message,
        "log_messages": task.log_messages[-50:],  # åªè¿”å›æœ€è¿‘50æ¡æ—¥å¿—
        # æ–°å¢åˆ†å—ä¿¡æ¯
        "current_chunk": task.current_chunk,
        "total_chunks": task.total_chunks,
        "chunks": [
            {
                "chunk_id": chunk.chunk_id,
                "start_page": chunk.start_page,
                "end_page": chunk.end_page,
                "page_count": chunk.page_count,
                "status": chunk.status
            }
            for chunk in (task.chunks or [])
        ] if task.chunks else [],
        "file_analysis": task.file_analysis,
        "split_config": task.split_config
    }


@app.post("/api/stop/{task_id}")
async def stop_processing(task_id: str):
    """åœæ­¢å¤„ç†ä»»åŠ¡"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    if task_id in task_stop_flags:
        task_stop_flags[task_id].set()
    
    task = tasks[task_id]
    if task.status == "processing":
        task.status = "stopped"
        task.end_time = datetime.now()
        add_log_message(task_id, "â¹ï¸ å¤„ç†å·²åœæ­¢", "warning")
    
    return {"message": "åœæ­¢ä¿¡å·å·²å‘é€"}


@app.get("/api/download/{filename}")
async def download_result(filename: str):
    """ä¸‹è½½å¤„ç†ç»“æœ"""
    output_dir = Path("ocr_output")
    file_path = output_dir / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åè®¾ç½®æ­£ç¡®çš„MIMEç±»å‹
    if filename.endswith('.md'):
        media_type = 'text/markdown'
    else:
        media_type = 'application/octet-stream'
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type=media_type
    )


@app.get("/api/results")
async def list_results():
    """è·å–å¤„ç†ç»“æœåˆ—è¡¨"""
    try:
        output_dir = Path("ocr_output")
        if not output_dir.exists():
            return {"files": []}
        
        result_files = []
        for file_path in output_dir.glob("*.md"):
            try:
                size = file_path.stat().st_size
                modified_time = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                
                result_files.append({
                    "name": file_path.name,
                    "size": size,
                    "modified_time": modified_time
                })
            except Exception as e:
                print(f"Error reading result file info for {file_path}: {e}")
                continue
        
        return {"files": result_files}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»“æœåˆ—è¡¨å¤±è´¥: {str(e)}")


def add_log_message(task_id: str, message: str, level: str = "info"):
    """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
    if task_id in tasks:
        timestamp = datetime.now().isoformat()
        log_entry = {
            "timestamp": timestamp,
            "message": message,
            "level": level
        }
        tasks[task_id].log_messages.append(log_entry)
        
        # é™åˆ¶æ—¥å¿—æ•°é‡
        if len(tasks[task_id].log_messages) > 1000:
            tasks[task_id].log_messages = tasks[task_id].log_messages[-500:]


def create_progress_callback(task_id: str):
    """åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°"""
    def callback(msg_type, data, **kwargs):
        if task_id not in tasks:
            return
        
        task = tasks[task_id]
        
        if msg_type == 'page_start':
            page_idx, total_pages = data
            task.current_page = page_idx + 1
            task.total_pages = total_pages
            add_log_message(task_id, f"å¼€å§‹å¤„ç†ç¬¬ {page_idx + 1} é¡µ", "info")
            
        elif msg_type == 'chunk_start':
            chunk_idx, total_chunks = data
            task.current_chunk = chunk_idx + 1
            task.total_chunks = total_chunks
            add_log_message(task_id, f"å¼€å§‹å¤„ç†åˆ†å— {chunk_idx + 1}/{total_chunks}", "info")
            
        elif msg_type == 'chunk_complete':
            chunk_idx = data
            add_log_message(task_id, f"âœ… åˆ†å— {chunk_idx + 1} å¤„ç†å®Œæˆ", "success")
            
        elif msg_type == 'ocr_token':
            tokens = data
            token_count = tokens.get('total_tokens', 0)
            task.ocr_tokens += token_count
            task.total_tokens += token_count
            
            input_tokens = tokens.get('input_tokens', 0)
            output_tokens = tokens.get('output_tokens', 0)
            add_log_message(task_id, f"ğŸ“Š OCR Token: {token_count:,} (è¾“å…¥: {input_tokens:,}, è¾“å‡º: {output_tokens:,})", "token")
            
        elif msg_type == 'gemini_token':
            tokens = data
            token_count = tokens.get('total_tokens', 0)
            task.gemini_tokens += token_count
            task.total_tokens += token_count
            
            input_tokens = tokens.get('input_tokens', 0)
            output_tokens = tokens.get('output_tokens', 0)
            add_log_message(task_id, f"ğŸ“Š Gemini Token: {token_count:,} (è¾“å…¥: {input_tokens:,}, è¾“å‡º: {output_tokens:,})", "token")
            
        elif msg_type == 'page_complete':
            page_idx = data
            add_log_message(task_id, f"âœ… ç¬¬ {page_idx + 1} é¡µå¤„ç†å®Œæˆ", "success")
    
    return callback


async def process_file_background(task_id: str, filename: str, start_page: int, end_page: int, terminology_terms: str, ocr_service: str, enable_splitting: bool = True, preprocessing_config = None):
    """åå°å¤„ç†å•ä¸ªæ–‡ä»¶ - æ”¯æŒåˆ†å—å¤„ç†"""
    try:
        task = tasks[task_id]
        task.status = "analyzing"
        add_log_message(task_id, f"ğŸš€ å¼€å§‹åˆ†ææ–‡ä»¶: {filename}", "info")
        
        # è·å–PDFæ–‡ä»¶è·¯å¾„
        input_dir = Path("input")
        pdf_path = input_dir / filename
        
        if not pdf_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        
        progress_callback = create_progress_callback(task_id)
        
        # æ›´æ–°å½“å‰æ–‡ä»¶
        task.current_file = pdf_path.name
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²å¤„ç†
        if enable_splitting and task.split_config:
            await process_with_splitting(task_id, pdf_path, start_page, end_page, terminology_terms, ocr_service, progress_callback, preprocessing_config)
        else:
            await process_without_splitting(task_id, pdf_path, start_page, end_page, terminology_terms, ocr_service, progress_callback, preprocessing_config)
        
    except Exception as e:
        task = tasks[task_id]
        task.status = "failed"
        task.error_message = str(e)
        task.end_time = datetime.now()
        add_log_message(task_id, f"âŒ å¤„ç†å¤±è´¥: {str(e)}", "error")


async def process_without_splitting(task_id: str, pdf_path: Path, start_page: int, end_page: int, terminology_terms: str, ocr_service: str, progress_callback, preprocessing_config = None):
    """ä¸åˆ†å‰²çš„ä¼ ç»Ÿå¤„ç†æ–¹å¼"""
    task = tasks[task_id]
    task.status = "processing"
    add_log_message(task_id, f"æ­£åœ¨å¤„ç†: {pdf_path.name}", "info")
    
    # å¯¼å…¥å¤„ç†å‡½æ•°
    from main import process_single_file_with_progress_callback
    
    # æ£€æŸ¥åœæ­¢æ ‡å¿—
    if task_id in task_stop_flags and task_stop_flags[task_id].is_set():
        task.status = "stopped"
        add_log_message(task_id, "â¹ï¸ å¤„ç†å·²åœæ­¢", "warning")
        return
    
    # é‡ç½®é¡µé¢Tokenç»Ÿè®¡
    task.current_page = 0
    task.total_pages = 0
    
    try:
        # å¤„ç†æ–‡ä»¶
        result = process_single_file_with_progress_callback(
            pdf_path, start_page, end_page,
            terminology_terms, ocr_service,
            progress_callback, preprocessing_config
        )
        add_log_message(task_id, f"âœ… {pdf_path.name} å¤„ç†å®Œæˆ", "success")
        
    except Exception as e:
        add_log_message(task_id, f"âŒ {pdf_path.name} å¤„ç†å¤±è´¥: {str(e)}", "error")
        raise
    
    # å¤„ç†å®Œæˆ
    if task.status == "processing":
        task.status = "completed"
        task.progress = 100.0
        add_log_message(task_id, "ğŸ‰ æ–‡ä»¶å¤„ç†å®Œæˆï¼", "success")
    
    task.end_time = datetime.now()


async def process_with_splitting(task_id: str, pdf_path: Path, start_page: int, end_page: int, terminology_terms: str, ocr_service: str, progress_callback, preprocessing_config = None):
    """åˆ†å—å¤„ç†æ–¹å¼"""
    task = tasks[task_id]
    
    try:
        # åˆ›å»ºåˆ†å‰²å™¨é…ç½®
        split_config = task.split_config
        config = SplitConfig(
            strategy=SplitStrategy(split_config["strategy"]),
            max_pages_per_chunk=split_config["max_pages_per_chunk"],
            max_size_per_chunk_mb=split_config["max_size_per_chunk_mb"],
            max_memory_usage_mb=split_config["max_memory_usage_mb"],
            parallel_processing=split_config["enable_parallel"],
            preserve_structure=split_config["preserve_structure"]
        )
        
        # åˆ›å»ºåˆ†å‰²å™¨
        splitter = PDFSplitter(config)
        
        # åˆ†ææ–‡ä»¶
        add_log_message(task_id, "ğŸ” åˆ†æPDFæ–‡ä»¶ç‰¹å¾...", "info")
        analysis = splitter.analyze_pdf(pdf_path)
        task.file_analysis = analysis
        
        # åˆ›å»ºåˆ†å‰²è®¡åˆ’
        add_log_message(task_id, "ğŸ“‹ åˆ›å»ºåˆ†å‰²è®¡åˆ’...", "info")
        chunks = splitter.create_split_plan(pdf_path, analysis)
        
        # è¿‡æ»¤é¡µé¢èŒƒå›´
        filtered_chunks = []
        for chunk in chunks:
            # è®¡ç®—ä¸è¯·æ±‚é¡µé¢èŒƒå›´çš„äº¤é›†
            chunk_start = max(chunk.start_page, start_page)
            chunk_end = min(chunk.end_page, end_page)
            
            if chunk_start <= chunk_end:
                # åˆ›å»ºæ–°çš„chunkä¿¡æ¯
                new_chunk = ChunkInfo(
                    chunk_id=chunk.chunk_id,
                    start_page=chunk_start,
                    end_page=chunk_end,
                    page_count=chunk_end - chunk_start + 1,
                    estimated_size_mb=chunk.estimated_size_mb * (chunk_end - chunk_start + 1) / chunk.page_count
                )
                filtered_chunks.append(new_chunk)
        
        task.chunks = filtered_chunks
        task.total_chunks = len(filtered_chunks)
        
        if not filtered_chunks:
            raise ValueError("æ²¡æœ‰éœ€è¦å¤„ç†çš„é¡µé¢")
        
        add_log_message(task_id, f"ğŸ“Š åˆ†å‰²è®¡åˆ’: {len(filtered_chunks)} ä¸ªåˆ†å—", "info")
        
        # åˆ†å‰²PDFæ–‡ä»¶
        if analysis["needs_splitting"]:
            task.status = "splitting"
            add_log_message(task_id, "âœ‚ï¸ åˆ†å‰²PDFæ–‡ä»¶...", "info")
            chunks_with_files = splitter.split_pdf_file(pdf_path, filtered_chunks)
            task.chunks = chunks_with_files
        else:
            add_log_message(task_id, "ğŸ“„ æ–‡ä»¶æ— éœ€åˆ†å‰²ï¼Œç›´æ¥å¤„ç†", "info")
            for chunk in filtered_chunks:
                chunk.file_path = pdf_path
                chunk.status = "ready"
        
        # å¤„ç†åˆ†å—
        task.status = "processing"
        await process_chunks(task_id, task.chunks, terminology_terms, ocr_service, progress_callback, splitter, preprocessing_config)
        
    except Exception as e:
        add_log_message(task_id, f"âŒ åˆ†å—å¤„ç†å¤±è´¥: {str(e)}", "error")
        raise


async def process_chunks(task_id: str, chunks: List[ChunkInfo], terminology_terms: str, ocr_service: str, progress_callback, splitter: PDFSplitter, preprocessing_config = None):
    """å¤„ç†æ‰€æœ‰åˆ†å—"""
    task = tasks[task_id]
    
    # å¯¼å…¥å¤„ç†å‡½æ•°
    from main import process_single_file_with_progress_callback
    
    successful_chunks = []
    failed_chunks = []
    
    for i, chunk in enumerate(chunks):
        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        if task_id in task_stop_flags and task_stop_flags[task_id].is_set():
            task.status = "stopped"
            add_log_message(task_id, "â¹ï¸ å¤„ç†å·²åœæ­¢", "warning")
            return
        
        # æ›´æ–°è¿›åº¦
        task.current_chunk = i + 1
        progress_callback('chunk_start', (i, len(chunks)))
        
        chunk.status = "processing"
        
        try:
            # å¤„ç†å•ä¸ªåˆ†å—
            add_log_message(task_id, f"ğŸ”„ å¤„ç†åˆ†å— {chunk.chunk_id} (é¡µé¢ {chunk.start_page}-{chunk.end_page})", "info")
            
            result = process_single_file_with_progress_callback(
                chunk.file_path, 
                chunk.start_page if chunk.file_path != chunks[0].file_path or len(chunks) == 1 else 1,  # å¦‚æœæ˜¯åˆ†å‰²æ–‡ä»¶ï¼Œä»ç¬¬1é¡µå¼€å§‹
                chunk.end_page if chunk.file_path != chunks[0].file_path or len(chunks) == 1 else chunk.page_count,
                terminology_terms, 
                ocr_service,
                progress_callback, preprocessing_config
            )
            
            chunk.status = "completed"
            chunk.ocr_result = result  # è¿™é‡Œéœ€è¦ä»å¤„ç†ç»“æœä¸­æå–æ–‡æœ¬
            successful_chunks.append(chunk)
            
            progress_callback('chunk_complete', i)
            add_log_message(task_id, f"âœ… åˆ†å— {chunk.chunk_id} å¤„ç†å®Œæˆ", "success")
            
        except Exception as e:
            chunk.status = "failed"
            chunk.error_message = str(e)
            failed_chunks.append(chunk)
            add_log_message(task_id, f"âŒ åˆ†å— {chunk.chunk_id} å¤„ç†å¤±è´¥: {str(e)}", "error")
        
        # æ›´æ–°æ€»è¿›åº¦
        task.progress = ((i + 1) / len(chunks)) * 90  # 90%ç”¨äºå¤„ç†ï¼Œ10%ç”¨äºåˆå¹¶
    
    # åˆå¹¶ç»“æœ
    if successful_chunks:
        task.status = "merging"
        add_log_message(task_id, "ğŸ”— åˆå¹¶å¤„ç†ç»“æœ...", "info")
        
        try:
            # è¯»å–OCRç»“æœæ–‡ä»¶
            for chunk in successful_chunks:
                if not chunk.ocr_result:
                    # ä»è¾“å‡ºæ–‡ä»¶ä¸­è¯»å–ç»“æœ
                    output_dir = Path("ocr_output")
                    if chunk.file_path:
                        base_name = chunk.file_path.stem
                        combined_file = output_dir / f"{base_name}_combined.md"
                        if combined_file.exists():
                            chunk.ocr_result = combined_file.read_text(encoding='utf-8')
            
            # åˆå¹¶ç»“æœ
            merged_result = splitter.merge_ocr_results(successful_chunks)
            
            # ä¿å­˜åˆå¹¶ç»“æœ
            output_dir = Path("ocr_output")
            output_dir.mkdir(exist_ok=True)
            final_output = output_dir / f"{chunks[0].file_path.stem}_merged_combined.md"
            final_output.write_text(merged_result, encoding='utf-8')
            
            add_log_message(task_id, f"ğŸ“ åˆå¹¶ç»“æœå·²ä¿å­˜: {final_output.name}", "success")
            
        except Exception as e:
            add_log_message(task_id, f"âš ï¸ ç»“æœåˆå¹¶å¤±è´¥ï¼Œä½†åˆ†å—å¤„ç†å·²å®Œæˆ: {str(e)}", "warning")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try:
        splitter.cleanup_chunks(chunks)
    except Exception as e:
        add_log_message(task_id, f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}", "warning")
    
    # å®Œæˆå¤„ç†
    if task.status == "merging":
        task.status = "completed"
        task.progress = 100.0
        add_log_message(task_id, f"ğŸ‰ æ–‡ä»¶å¤„ç†å®Œæˆï¼æˆåŠŸ: {len(successful_chunks)}, å¤±è´¥: {len(failed_chunks)}", "success")
    
    task.end_time = datetime.now()


if __name__ == "__main__":
    # ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
    Path("input").mkdir(exist_ok=True)
    Path("ocr_output").mkdir(exist_ok=True)
    Path("web").mkdir(exist_ok=True)
    
    print("ğŸš€ å¯åŠ¨OCR APIæœåŠ¡å™¨...")
    print("ğŸ“ APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸŒ Webç•Œé¢: http://localhost:8000")
    
    uvicorn.run(
        "backend:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )