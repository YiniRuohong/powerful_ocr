#!/usr/bin/env python3
"""
æ™ºèƒ½ç¼“å­˜ç®¡ç†ç³»ç»Ÿ
æä¾›æ–‡æ¡£OCRç»“æœçš„ç¼“å­˜ã€é‡è¯•æœºåˆ¶å’Œå­˜å‚¨ç®¡ç†åŠŸèƒ½
"""

import os
import json
import hashlib
import pickle
import time
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import tempfile

from PIL import Image


class CacheStatus(Enum):
    """ç¼“å­˜çŠ¶æ€æšä¸¾"""
    PENDING = "pending"           # ç­‰å¾…å¤„ç†
    PROCESSING = "processing"     # æ­£åœ¨å¤„ç†
    COMPLETED = "completed"       # å¤„ç†å®Œæˆ
    FAILED = "failed"            # å¤„ç†å¤±è´¥
    EXPIRED = "expired"          # å·²è¿‡æœŸ
    CORRUPTED = "corrupted"      # ç¼“å­˜æŸå


class RetryStrategy(Enum):
    """é‡è¯•ç­–ç•¥æšä¸¾"""
    NONE = "none"                # ä¸é‡è¯•
    SIMPLE = "simple"            # ç®€å•é‡è¯•
    EXPONENTIAL = "exponential"   # æŒ‡æ•°é€€é¿
    SMART = "smart"              # æ™ºèƒ½é‡è¯•


@dataclass
class CacheMetadata:
    """ç¼“å­˜å…ƒæ•°æ®"""
    file_path: str
    file_hash: str
    file_size: int
    file_mtime: float
    cache_key: str
    cache_version: str
    created_time: datetime
    last_accessed: datetime
    access_count: int
    processing_config: Dict[str, Any]
    ocr_service: str
    terminology_hash: str
    page_range: Tuple[int, int]
    status: CacheStatus
    error_message: Optional[str] = None
    retry_count: int = 0
    max_retries: int = 3
    processing_time: float = 0.0
    output_size: int = 0


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    cache_dir: Path = Path("cache")
    max_cache_size_gb: float = 5.0        # æœ€å¤§ç¼“å­˜å¤§å°(GB)
    max_cache_age_days: int = 30          # æœ€å¤§ç¼“å­˜ä¿å­˜å¤©æ•°
    max_entries: int = 1000               # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
    cleanup_interval_hours: int = 24      # æ¸…ç†é—´éš”(å°æ—¶)
    enable_compression: bool = True       # å¯ç”¨å‹ç¼©
    cache_version: str = "v1.0"          # ç¼“å­˜ç‰ˆæœ¬
    auto_cleanup: bool = True            # è‡ªåŠ¨æ¸…ç†
    preserve_recent: int = 100           # ä¿ç•™æœ€è¿‘è®¿é—®çš„æ¡ç›®æ•°


class CacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: CacheConfig = None):
        self.config = config or CacheConfig()
        self.cache_dir = self.config.cache_dir
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.lock = threading.RLock()
        
        # åˆå§‹åŒ–ç¼“å­˜ç›®å½•
        self._init_cache_dir()
        
        # åŠ è½½å…ƒæ•°æ®
        self.metadata: Dict[str, CacheMetadata] = self._load_metadata()
        
        # å¯åŠ¨å®šæœŸæ¸…ç†
        if self.config.auto_cleanup:
            self._start_cleanup_scheduler()
    
    def _init_cache_dir(self):
        """åˆå§‹åŒ–ç¼“å­˜ç›®å½•ç»“æ„"""
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.cache_dir / "data").mkdir(exist_ok=True)      # ç¼“å­˜æ•°æ®
        (self.cache_dir / "temp").mkdir(exist_ok=True)      # ä¸´æ—¶æ–‡ä»¶
        (self.cache_dir / "backup").mkdir(exist_ok=True)    # å¤‡ä»½æ–‡ä»¶
        (self.cache_dir / "logs").mkdir(exist_ok=True)      # æ—¥å¿—æ–‡ä»¶
    
    def _generate_cache_key(self, file_path: Path, config: Dict[str, Any], 
                           ocr_service: str, terminology_terms: str, 
                           page_range: Tuple[int, int]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = self._calculate_file_hash(file_path)
        
        # è®¡ç®—é…ç½®å“ˆå¸Œ
        config_str = json.dumps(config, sort_keys=True) if config else ""
        terminology_hash = hashlib.md5(terminology_terms.encode()).hexdigest()
        
        # ç»„åˆæ‰€æœ‰å‚æ•°
        key_data = {
            "file_hash": file_hash,
            "config": config_str,
            "ocr_service": ocr_service,
            "terminology": terminology_hash,
            "page_range": page_range,
            "version": self.config.cache_version
        }
        
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.sha256(key_str.encode()).hexdigest()
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            hash_md5 = hashlib.md5()
            
            # å¯¹äºå¤§æ–‡ä»¶ï¼Œåªè®¡ç®—å‰1MB + æ–‡ä»¶ä¿¡æ¯çš„å“ˆå¸Œ
            file_stat = file_path.stat()
            
            # æ·»åŠ æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
            file_info = f"{file_path.name}:{file_stat.st_size}:{file_stat.st_mtime}"
            hash_md5.update(file_info.encode())
            
            # è¯»å–æ–‡ä»¶å¼€å¤´éƒ¨åˆ†
            with open(file_path, 'rb') as f:
                chunk_size = min(1024 * 1024, file_stat.st_size)  # æœ€å¤š1MB
                chunk = f.read(chunk_size)
                hash_md5.update(chunk)
            
            return hash_md5.hexdigest()
        except Exception as e:
            # å¦‚æœæ— æ³•è¯»å–æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶ä¿¡æ¯
            file_stat = file_path.stat()
            fallback_info = f"{file_path.name}:{file_stat.st_size}:{file_stat.st_mtime}"
            return hashlib.md5(fallback_info.encode()).hexdigest()
    
    def _load_metadata(self) -> Dict[str, CacheMetadata]:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if not self.metadata_file.exists():
            return {}
        
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            metadata = {}
            for key, item in data.items():
                try:
                    # è½¬æ¢æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
                    item['created_time'] = datetime.fromisoformat(item['created_time'])
                    item['last_accessed'] = datetime.fromisoformat(item['last_accessed'])
                    item['status'] = CacheStatus(item['status'])
                    
                    metadata[key] = CacheMetadata(**item)
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æŸåçš„ç¼“å­˜å…ƒæ•°æ®é¡¹: {key} - {e}")
                    continue
            
            return metadata
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        try:
            # å¤‡ä»½ç°æœ‰å…ƒæ•°æ®
            if self.metadata_file.exists():
                backup_file = self.cache_dir / "backup" / f"metadata_{int(time.time())}.json"
                shutil.copy2(self.metadata_file, backup_file)
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            data = {}
            for key, metadata in self.metadata.items():
                item = asdict(metadata)
                item['created_time'] = metadata.created_time.isoformat()
                item['last_accessed'] = metadata.last_accessed.isoformat()
                item['status'] = metadata.status.value
                data[key] = item
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶ååŸå­æ€§æ›¿æ¢
            temp_file = self.metadata_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            temp_file.replace(self.metadata_file)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _get_cache_path(self, cache_key: str, suffix: str = ".pkl") -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨å“ˆå¸Œçš„å‰ä¸¤ä½ä½œä¸ºå­ç›®å½•ï¼Œé¿å…å•ç›®å½•æ–‡ä»¶è¿‡å¤š
        subdir = cache_key[:2]
        cache_subdir = self.cache_dir / "data" / subdir
        cache_subdir.mkdir(exist_ok=True)
        return cache_subdir / f"{cache_key}{suffix}"
    
    def has_cache(self, file_path: Path, config: Dict[str, Any], 
                  ocr_service: str, terminology_terms: str, 
                  page_range: Tuple[int, int]) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ‰æ•ˆç¼“å­˜"""
        with self.lock:
            cache_key = self._generate_cache_key(
                file_path, config, ocr_service, terminology_terms, page_range
            )
            
            if cache_key not in self.metadata:
                return False
            
            metadata = self.metadata[cache_key]
            
            # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
            if metadata.status != CacheStatus.COMPLETED:
                return False
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨ä¸”æœªä¿®æ”¹
            if not file_path.exists():
                return False
            
            file_stat = file_path.stat()
            if (file_stat.st_size != metadata.file_size or 
                abs(file_stat.st_mtime - metadata.file_mtime) > 1):
                return False
            
            # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            cache_path = self._get_cache_path(cache_key)
            if not cache_path.exists():
                return False
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if self._is_expired(metadata):
                return False
            
            return True
    
    def get_cache(self, file_path: Path, config: Dict[str, Any], 
                  ocr_service: str, terminology_terms: str, 
                  page_range: Tuple[int, int]) -> Optional[str]:
        """è·å–ç¼“å­˜ç»“æœ"""
        with self.lock:
            if not self.has_cache(file_path, config, ocr_service, terminology_terms, page_range):
                return None
            
            cache_key = self._generate_cache_key(
                file_path, config, ocr_service, terminology_terms, page_range
            )
            
            try:
                cache_path = self._get_cache_path(cache_key)
                
                if self.config.enable_compression:
                    import gzip
                    with gzip.open(cache_path, 'rb') as f:
                        result = pickle.load(f)
                else:
                    with open(cache_path, 'rb') as f:
                        result = pickle.load(f)
                
                # æ›´æ–°è®¿é—®ä¿¡æ¯
                metadata = self.metadata[cache_key]
                metadata.last_accessed = datetime.now()
                metadata.access_count += 1
                
                self._save_metadata()
                
                print(f"âœ… ä»ç¼“å­˜åŠ è½½ç»“æœ: {cache_key[:8]}... (è®¿é—®æ¬¡æ•°: {metadata.access_count})")
                return result
                
            except Exception as e:
                print(f"âŒ è¯»å–ç¼“å­˜å¤±è´¥: {e}")
                # æ ‡è®°ç¼“å­˜ä¸ºæŸå
                if cache_key in self.metadata:
                    self.metadata[cache_key].status = CacheStatus.CORRUPTED
                    self._save_metadata()
                return None
    
    def set_cache(self, file_path: Path, config: Dict[str, Any], 
                  ocr_service: str, terminology_terms: str, 
                  page_range: Tuple[int, int], result: str,
                  processing_time: float = 0.0) -> bool:
        """è®¾ç½®ç¼“å­˜"""
        with self.lock:
            try:
                cache_key = self._generate_cache_key(
                    file_path, config, ocr_service, terminology_terms, page_range
                )
                
                # ä¿å­˜ç¼“å­˜æ•°æ®
                cache_path = self._get_cache_path(cache_key)
                
                if self.config.enable_compression:
                    import gzip
                    with gzip.open(cache_path, 'wb') as f:
                        pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)
                else:
                    with open(cache_path, 'wb') as f:
                        pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)
                
                # åˆ›å»ºå…ƒæ•°æ®
                file_stat = file_path.stat()
                terminology_hash = hashlib.md5(terminology_terms.encode()).hexdigest()
                
                metadata = CacheMetadata(
                    file_path=str(file_path),
                    file_hash=self._calculate_file_hash(file_path),
                    file_size=file_stat.st_size,
                    file_mtime=file_stat.st_mtime,
                    cache_key=cache_key,
                    cache_version=self.config.cache_version,
                    created_time=datetime.now(),
                    last_accessed=datetime.now(),
                    access_count=0,
                    processing_config=config or {},
                    ocr_service=ocr_service,
                    terminology_hash=terminology_hash,
                    page_range=page_range,
                    status=CacheStatus.COMPLETED,
                    processing_time=processing_time,
                    output_size=len(result.encode('utf-8'))
                )
                
                self.metadata[cache_key] = metadata
                self._save_metadata()
                
                print(f"ğŸ’¾ ç¼“å­˜ä¿å­˜æˆåŠŸ: {cache_key[:8]}... ({len(result)} å­—ç¬¦)")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
                if len(self.metadata) > self.config.max_entries:
                    self._cleanup_old_entries()
                
                return True
                
            except Exception as e:
                print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
                return False
    
    def mark_processing(self, file_path: Path, config: Dict[str, Any], 
                       ocr_service: str, terminology_terms: str, 
                       page_range: Tuple[int, int]) -> str:
        """æ ‡è®°æ–‡ä»¶æ­£åœ¨å¤„ç†"""
        with self.lock:
            cache_key = self._generate_cache_key(
                file_path, config, ocr_service, terminology_terms, page_range
            )
            
            file_stat = file_path.stat()
            terminology_hash = hashlib.md5(terminology_terms.encode()).hexdigest()
            
            metadata = CacheMetadata(
                file_path=str(file_path),
                file_hash=self._calculate_file_hash(file_path),
                file_size=file_stat.st_size,
                file_mtime=file_stat.st_mtime,
                cache_key=cache_key,
                cache_version=self.config.cache_version,
                created_time=datetime.now(),
                last_accessed=datetime.now(),
                access_count=0,
                processing_config=config or {},
                ocr_service=ocr_service,
                terminology_hash=terminology_hash,
                page_range=page_range,
                status=CacheStatus.PROCESSING
            )
            
            self.metadata[cache_key] = metadata
            self._save_metadata()
            
            return cache_key
    
    def mark_failed(self, cache_key: str, error_message: str):
        """æ ‡è®°å¤„ç†å¤±è´¥"""
        with self.lock:
            if cache_key in self.metadata:
                metadata = self.metadata[cache_key]
                metadata.status = CacheStatus.FAILED
                metadata.error_message = error_message
                metadata.retry_count += 1
                self._save_metadata()
    
    def can_retry(self, cache_key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•"""
        with self.lock:
            if cache_key not in self.metadata:
                return True
            
            metadata = self.metadata[cache_key]
            return metadata.retry_count < metadata.max_retries
    
    def _is_expired(self, metadata: CacheMetadata) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        age = datetime.now() - metadata.created_time
        return age > timedelta(days=self.config.max_cache_age_days)
    
    def cleanup(self) -> Dict[str, int]:
        """æ¸…ç†ç¼“å­˜"""
        with self.lock:
            stats = {
                "removed_expired": 0,
                "removed_corrupted": 0,
                "removed_old": 0,
                "removed_failed": 0,
                "total_removed": 0,
                "bytes_freed": 0
            }
            
            to_remove = []
            
            # æ£€æŸ¥æ¯ä¸ªç¼“å­˜é¡¹
            for cache_key, metadata in self.metadata.items():
                should_remove = False
                cache_path = self._get_cache_path(cache_key)
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if self._is_expired(metadata):
                    should_remove = True
                    stats["removed_expired"] += 1
                
                # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                elif not cache_path.exists():
                    should_remove = True
                    stats["removed_corrupted"] += 1
                
                # æ£€æŸ¥çŠ¶æ€
                elif metadata.status in [CacheStatus.CORRUPTED, CacheStatus.FAILED]:
                    # å¤±è´¥çš„é¡¹ç›®å¦‚æœé‡è¯•æ¬¡æ•°ç”¨å®Œåˆ™åˆ é™¤
                    if metadata.status == CacheStatus.FAILED and metadata.retry_count >= metadata.max_retries:
                        should_remove = True
                        stats["removed_failed"] += 1
                    elif metadata.status == CacheStatus.CORRUPTED:
                        should_remove = True
                        stats["removed_corrupted"] += 1
                
                if should_remove:
                    to_remove.append(cache_key)
                    if cache_path.exists():
                        try:
                            stats["bytes_freed"] += cache_path.stat().st_size
                        except:
                            pass
            
            # å¦‚æœæ¡ç›®æ•°ä»ç„¶è¿‡å¤šï¼Œåˆ é™¤æœ€æ—§çš„
            remaining_items = len(self.metadata) - len(to_remove)
            if remaining_items > self.config.max_entries:
                # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åºï¼Œä¿ç•™æœ€è¿‘çš„
                sorted_items = sorted(
                    [(k, v) for k, v in self.metadata.items() if k not in to_remove],
                    key=lambda x: x[1].last_accessed,
                    reverse=True
                )
                
                keep_count = min(self.config.max_entries, self.config.preserve_recent)
                for cache_key, metadata in sorted_items[keep_count:]:
                    to_remove.append(cache_key)
                    stats["removed_old"] += 1
            
            # åˆ é™¤ç¼“å­˜é¡¹
            for cache_key in to_remove:
                try:
                    cache_path = self._get_cache_path(cache_key)
                    if cache_path.exists():
                        cache_path.unlink()
                    
                    if cache_key in self.metadata:
                        del self.metadata[cache_key]
                    
                    stats["total_removed"] += 1
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤ç¼“å­˜é¡¹å¤±è´¥ {cache_key}: {e}")
            
            if stats["total_removed"] > 0:
                self._save_metadata()
                print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤ {stats['total_removed']} é¡¹, é‡Šæ”¾ {stats['bytes_freed']/1024/1024:.1f}MB")
            
            return stats
    
    def _cleanup_old_entries(self):
        """æ¸…ç†æ—§æ¡ç›®"""
        if len(self.metadata) <= self.config.max_entries:
            return
        
        # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
        sorted_items = sorted(
            self.metadata.items(),
            key=lambda x: x[1].last_accessed
        )
        
        remove_count = len(self.metadata) - self.config.max_entries
        
        for i in range(remove_count):
            cache_key, metadata = sorted_items[i]
            
            try:
                cache_path = self._get_cache_path(cache_key)
                if cache_path.exists():
                    cache_path.unlink()
                
                del self.metadata[cache_key]
                
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤æ—§ç¼“å­˜é¡¹å¤±è´¥ {cache_key}: {e}")
    
    def _start_cleanup_scheduler(self):
        """å¯åŠ¨å®šæœŸæ¸…ç†è°ƒåº¦å™¨"""
        def cleanup_worker():
            while True:
                try:
                    time.sleep(self.config.cleanup_interval_hours * 3600)
                    self.cleanup()
                except Exception as e:
                    print(f"âš ï¸ å®šæœŸæ¸…ç†å¤±è´¥: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_size = 0
            status_counts = {}
            oldest_cache = None
            newest_cache = None
            total_processing_time = 0
            total_access_count = 0
            
            for metadata in self.metadata.values():
                # è®¡ç®—å¤§å°
                cache_path = self._get_cache_path(metadata.cache_key)
                if cache_path.exists():
                    total_size += cache_path.stat().st_size
                
                # ç»Ÿè®¡çŠ¶æ€
                status = metadata.status.value
                status_counts[status] = status_counts.get(status, 0) + 1
                
                # æ‰¾æœ€æ—§å’Œæœ€æ–°çš„ç¼“å­˜
                if oldest_cache is None or metadata.created_time < oldest_cache:
                    oldest_cache = metadata.created_time
                if newest_cache is None or metadata.created_time > newest_cache:
                    newest_cache = metadata.created_time
                
                # ç´¯è®¡ç»Ÿè®¡
                total_processing_time += metadata.processing_time
                total_access_count += metadata.access_count
            
            return {
                "total_entries": len(self.metadata),
                "total_size_mb": total_size / 1024 / 1024,
                "status_counts": status_counts,
                "oldest_cache": oldest_cache.isoformat() if oldest_cache else None,
                "newest_cache": newest_cache.isoformat() if newest_cache else None,
                "total_processing_time": total_processing_time,
                "total_access_count": total_access_count,
                "config": asdict(self.config)
            }
    
    def clear_all(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        with self.lock:
            try:
                # åˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
                data_dir = self.cache_dir / "data"
                if data_dir.exists():
                    shutil.rmtree(data_dir)
                    data_dir.mkdir(exist_ok=True)
                
                # æ¸…ç©ºå…ƒæ•°æ®
                self.metadata.clear()
                self._save_metadata()
                
                print("ğŸ—‘ï¸ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")
                return True
                
            except Exception as e:
                print(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
                return False


def create_cache_manager(max_cache_size_gb: float = 5.0, 
                        max_cache_age_days: int = 30) -> CacheManager:
    """åˆ›å»ºç¼“å­˜ç®¡ç†å™¨çš„å·¥å‚å‡½æ•°"""
    config = CacheConfig(
        max_cache_size_gb=max_cache_size_gb,
        max_cache_age_days=max_cache_age_days
    )
    return CacheManager(config)


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_global_cache_manager = None


def get_cache_manager() -> CacheManager:
    """è·å–å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _global_cache_manager
    if _global_cache_manager is None:
        _global_cache_manager = create_cache_manager()
    return _global_cache_manager